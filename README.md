# VLM Object Recognition System (3-Mode)

A comprehensive multi-modal Vision Language Model (VLM) system that supports object detection and localization through three different processing pathways: cloud-based Grok-4, cloud-based Qwen-VL, and local LLaVA via Ollama.

## ğŸŒŸ Features

### Multi-VLM Support
- **â˜ï¸ Grok-4** - High accuracy cloud processing via X.AI API
- **â˜ï¸ Qwen-VL** - Excellent Chinese language support via DashScope API  
- **ğŸ–¥ï¸ LLaVA** - Privacy-focused local processing via Ollama

### Language Support
- **ğŸŒ Bilingual Commands** - Supports both English and Chinese input
- **ğŸ”„ Auto-Translation** - Automatic Chinese-to-English translation with pattern matching
- **ğŸ¯ Smart Object Extraction** - Intelligent parsing of natural language commands

### Advanced Processing
- **ğŸ“ Original Resolution** - Processes images at full resolution for maximum accuracy
- **ğŸ¯ Precise Coordinates** - Pixel-perfect object localization with visual markers
- **ğŸ”Š Audio Feedback** - Text-to-speech responses using macOS built-in `say` command
- **ğŸ–¼ï¸ Visual Annotation** - Displays detected objects with yellow star markers

### System Architecture
- **ğŸ”§ Modular Design** - Clean separation between VLM providers
- **âš¡ Smart Fallbacks** - Graceful error handling and alternative pathways
- **ğŸ“Š Performance Monitoring** - Detailed timing and processing statistics

## ğŸš€ Quick Start

### Prerequisites

1. **Python Dependencies**
```bash
pip install requests pillow openai python-dotenv
```

2. **Environment Variables**
```bash
export XAI_API_KEY="your_x_ai_api_key"          # For Grok-4
export DASHSCOPE_API_KEY="your_dashscope_key"   # For Qwen-VL
```

3. **Local Processing (Optional)**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Install LLaVA model
ollama pull llava:latest

# Start Ollama service
ollama serve
```

### Basic Usage

```bash
python3 imageRecogVLM.py
```

**English Commands:**
```
"please grab the apple to me"
"find the red coke bottle"
"identify the book on the shelf"
```

**Chinese Commands:**
```
"è¯·å¸®æˆ‘æ‹¿å¯ä¹ç»™æˆ‘"
"æ‰¾è‹¹æœç»™æˆ‘"
"å¸®æˆ‘æ‰¾ä¹¦"
```

## ğŸ—ï¸ System Architecture

### Processing Flow
1. **Input Processing** - Language detection and translation
2. **VLM Selection** - Interactive choice between 3 processing modes
3. **Image Encoding** - High-quality base64 encoding at original resolution
4. **API Calling** - Provider-specific prompt optimization and API calls
5. **Response Parsing** - Coordinate extraction and validation
6. **Output Generation** - Comprehensive response with visual and audio feedback

### VLM Comparison

| Feature | Grok-4 | Qwen-VL | LLaVA (Local) |
|---------|---------|---------|---------------|
| **Accuracy** | High | Good | Moderate |
| **Speed** | Medium | Medium | Fast |
| **Cost** | Paid | Paid | Free |
| **Privacy** | Cloud | Cloud | Local |
| **Chinese Support** | Basic | Excellent | Limited |
| **Internet Required** | Yes | Yes | No |

## ğŸ“ Project Structure

```
vlmTry/
â”œâ”€â”€ imageRecogVLM.py          # Main application
â”œâ”€â”€ sampleImages/             # Test images directory
â”‚   â”œâ”€â”€ image_000078.jpg
â”‚   â””â”€â”€ image_000354.jpg
â”œâ”€â”€ hello_qwen.py            # Qwen API test script
â”œâ”€â”€ README.md                # This file
â””â”€â”€ system_arch_new.mmd      # System architecture diagram
```

## ğŸ”§ Configuration

### Image Processing
- **Resolution**: Uses original image resolution for maximum accuracy
- **Quality**: High-quality JPEG encoding (95% for originals)
- **Format**: Base64 encoding for API transmission

### API Endpoints
- **Grok-4**: `https://api.x.ai/v1/chat/completions`
- **Qwen-VL**: `https://dashscope.aliyuncs.com/compatible-mode/v1`
- **LLaVA**: `http://localhost:11434/api/generate` (local)

## ğŸŒ Chinese Language Support

### Supported Patterns

**Command Patterns:**
- `è¯·.*?æ‹¿.*?ç»™æˆ‘` â†’ "please grab {} to me"
- `å¸®æˆ‘.*?æ‹¿.*` â†’ "help me get {}"
- `æ‰¾.*?ç»™æˆ‘` â†’ "find {} for me"
- `ç»™æˆ‘.*?æ‹¿` â†’ "get me {}"
- `è¯·.*?æ‰¾` â†’ "please find {}"
- `å¸®æˆ‘.*?æ‰¾` â†’ "help me find {}"

**Object Vocabulary:**
- `å¯ä¹|å¯å£å¯ä¹` â†’ "coke"
- `è‹¹æœ` â†’ "apple"
- `ä¹¦|ä¹¦æœ¬` â†’ "book"
- `ç“¶å­|æ°´ç“¶` â†’ "bottle"
- `é’¥åŒ™` â†’ "keys"
- `æ‰‹æœº|ç”µè¯` â†’ "phone"
- And more...

### Adding New Vocabulary

To extend Chinese support, edit the `chinese_patterns` dictionary in `translate_chinese_to_english()`:

```python
chinese_patterns = {
    # Add new command patterns
    r'æˆ‘æƒ³è¦.*': 'I want {}',
    
    # Add new object translations
    r'æ©™å­|æ©˜å­': 'orange',
    r'é¦™è•‰': 'banana',
}
```

## ğŸ“Š Output Format

### Console Output
```
ğŸ¤– VLM Object Recognition System (3-Mode)
============================================================
ğŸ• Process started at: 2025-01-16 14:30:25

ğŸ’¬ Enter your command: è¯·å¸®æˆ‘æ‹¿å¯ä¹ç»™æˆ‘

ğŸŒ Original Chinese command: 'è¯·å¸®æˆ‘æ‹¿å¯ä¹ç»™æˆ‘'
ğŸ”„ Translated English command: 'please grab coke to me'
âœ… Using translated command for processing

ğŸ¯ Target object identified: 'coke'
ğŸ“‚ Loading image: image_000354.jpg
   âœ“ Image loaded successfully: 1024x768

ğŸ”§ Building prompt for VLM...
   âœ“ Using resolution: 1024x768 (original size)

[VLM Selection Menu]
Choose processing mode (1 for Grok, 2 for Qwen, 3 for Local): 2

ğŸš€ Calling Qwen-VL Vision API (Cloud)...
ğŸ“¡ Qwen API response received in 2.34 seconds
âœ… Total Qwen API process completed in 3.12 seconds

ğŸ” Starting coordinate parsing...
   ğŸ“Š No scaling needed - using original coordinates
   ğŸ¯ Row 1: Direct coordinates(520,340) [ID: 1]
âœ… Successfully extracted 1 valid coordinate(s)

ğŸ“¬ Response:
==================================================
coke is recognized, let me fetch it to you

==================================================
ğŸ“„ ORIGINAL QWEN-VL MODEL OUTPUT:
==================================================
[VLM response content]
==================================================

ğŸ“Š COORDINATE SUMMARY TABLE:
----------------------------------------
| Object ID | H (Horizontal) | V (Vertical) |
|-----------|----------------|--------------|
|     1     |        520     |      340     |
----------------------------------------

ğŸ”Š Audio played: 'coke found'
ğŸ–¼ï¸ Showing image with annotated object location...
âœ… Process ended at: 2025-01-16 14:30:32
```

### Visual Output
- **Yellow star markers** placed at detected object center points
- **Image display** with annotation overlay
- **Coordinate validation** ensures markers are within image bounds

## ğŸ› ï¸ Troubleshooting

### Common Issues

**API Key Errors:**
```bash
# Set environment variables
export XAI_API_KEY="your_key_here"
export DASHSCOPE_API_KEY="your_key_here"
```

**Ollama Connection Issues:**
```bash
# Check Ollama status
ollama list

# Restart Ollama service
ollama serve

# Install missing models
ollama pull llava:latest
```

**Audio Issues:**
- Uses macOS built-in `say` command
- Automatically falls back to text-only if TTS fails
- Can be disabled by setting `tts_enabled = False`

### Performance Optimization

**For High Accuracy:**
- Use Grok-4 for best results
- Use original image resolution
- Provide specific object descriptions

**For Speed:**
- Use local LLaVA processing
- Enable image resizing if needed
- Use shorter, simpler commands

**For Chinese Processing:**
- Use Qwen-VL for best Chinese language support
- Expand `chinese_patterns` for domain-specific vocabulary
- Test translations before processing

## ğŸ“ˆ Performance Metrics

Typical processing times:
- **Grok-4**: 3-8 seconds (depending on image size)
- **Qwen-VL**: 2-5 seconds (good optimization)
- **LLaVA Local**: 1-3 seconds (fastest, hardware dependent)

## ğŸ¤ Contributing

To extend the system:

1. **Add new VLM providers** by implementing the API call pattern
2. **Expand language support** by adding patterns to translation functions
3. **Improve object detection** by optimizing prompts for specific use cases
4. **Add new output formats** by extending the response generation

## ğŸ“„ License

This project is open source. Please ensure you comply with the terms of service for any commercial VLM APIs you use.

## ğŸ™ Acknowledgments

- **X.AI** for Grok-4 API access
- **Alibaba Cloud** for Qwen-VL via DashScope
- **Ollama** for local LLaVA processing infrastructure
- **OpenAI** for compatible API standards